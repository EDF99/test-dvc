{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenando a Chimi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero preparar el modelo base en preparacion_modelo.ipynb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Luego de elegir el kernel de python (recomendado un venv) para este notebook. Instalar los paquetes necesarios a ese kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynvml import *\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparar el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import MT5Tokenizer, MT5ForConditionalGeneration, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, pipeline \n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f3af201d5e43a68ed00ccc4f9078d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['frase', 'respuesta'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chimi_dataset = load_dataset(\"csv\", data_files=\"chimi_training_data_v1.csv\")\n",
    "chimi_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bec278f27344277a6b4b6dbc890861f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def modify_newline_char(row_data):\n",
    "    row_data[\"frase\"] = row_data[\"frase\"].replace(\"\\n\", \"|\")\n",
    "    return row_data\n",
    "\n",
    "chimi_dataset = chimi_dataset.map(modify_newline_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['frase', 'respuesta'],\n",
       "        num_rows: 4000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['frase', 'respuesta'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['frase', 'respuesta'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chimi_train_test = chimi_dataset[\"train\"].train_test_split(test_size=0.1)\n",
    "chimi_train_validation = chimi_train_test[\"train\"].train_test_split(test_size=chimi_train_test[\"test\"].num_rows)\n",
    "\n",
    "chimi_dataset[\"train\"] = chimi_train_validation[\"train\"]\n",
    "chimi_dataset[\"validation\"] = chimi_train_validation[\"test\"]\n",
    "chimi_dataset[\"test\"] = chimi_train_test[\"test\"]\n",
    "chimi_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33acad2db25c446cb8a2746ae0e02480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ffc1fb378f74a79ae7cc6d26eb83ff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78558efe581f4a68b0475f31360d8d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "60919"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chimi_dataset[\"train\"].to_csv(\"train.csv\")\n",
    "chimi_dataset[\"validation\"].to_csv(\"validation.csv\")\n",
    "chimi_dataset[\"test\"].to_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_base_path = \"models/spa-mt5\"\n",
    "tokenizer = MT5Tokenizer.from_pretrained(modelo_base_path, legacy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 512\n",
    "max_target_length = 100\n",
    "\n",
    "def tokenize_function(row_data):\n",
    "    # try:\n",
    "        model_inputs = tokenizer(row_data['frase'], max_length=max_input_length, truncation=True)\n",
    "        labels = tokenizer(row_data[\"respuesta\"], max_length=max_target_length, truncation=True)\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "    # except Exception as e:\n",
    "    #     print(e)\n",
    "    #     print(row_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "826495e7e0e54d19821f4b08dcbee9c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5d39ce556c04acb842cc74753b8c1cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0fdf5649d9d4a89b0a3c768db694c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['frase', 'respuesta', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 4000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['frase', 'respuesta', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['frase', 'respuesta', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_chimi_dataset = chimi_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_chimi_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = MT5ForConditionalGeneration.from_pretrained(modelo_base_path)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=base_model)\n",
    "cer = evaluate.load(\"cer\", module_type=\"metric\")\n",
    "exact_match = evaluate.load(\"exact_match\", module_type=\"metric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_eval(predictions, references):\n",
    "    agregar_contacto_len = 0\n",
    "    transferencia_len = 0\n",
    "    total_len = len(predictions)\n",
    "    correct_action = 0\n",
    "    correct_alias = 0\n",
    "    correct_nombre = 0\n",
    "    correct_monto = 0\n",
    "    correct_ent = 0\n",
    "    correct_cuenta = 0\n",
    "    correct_moneda = 0\n",
    "    correct_doc = 0\n",
    "    for prediction, reference in zip(predictions, references):\n",
    "        splitted_prediction = prediction.split(\"|\")\n",
    "        len_pred = len(splitted_prediction)\n",
    "        \n",
    "        splitted_reference = reference.split(\"|\")\n",
    "        reference_action = splitted_reference[0]\n",
    "\n",
    "        if(splitted_prediction[0] == reference_action):\n",
    "            correct_action += 1\n",
    "\n",
    "        if(reference_action == \"T\" or reference_action == \"A\"):\n",
    "            # T|alias|name|monto|entidad|nro_cuenta|moneda|nro_doc\n",
    "            # A|alias|name|monto|entidad|nro_cuenta|moneda|nro_doc\n",
    "            transferencia_len += 1\n",
    "            if(len_pred > 1 and splitted_prediction[1].lower() == splitted_reference[1].lower()):\n",
    "                correct_alias += 1\n",
    "            if(len_pred > 2 and splitted_prediction[2].lower() == splitted_reference[2].lower()):\n",
    "                correct_nombre += 1\n",
    "            if(len_pred > 3 and splitted_prediction[3].lower() == splitted_reference[3].lower()):\n",
    "                correct_monto += 1\n",
    "            if(len_pred > 4 and splitted_prediction[4].lower() == splitted_reference[4].lower()):\n",
    "                correct_ent += 1\n",
    "            if(len_pred > 5 and splitted_prediction[5].lower() == splitted_reference[5].lower()):\n",
    "                correct_cuenta += 1\n",
    "            if(len_pred > 6 and splitted_prediction[6].lower() == splitted_reference[6].lower()):\n",
    "                correct_moneda += 1\n",
    "            if(len_pred > 7 and splitted_prediction[7].lower() == splitted_reference[7].lower()):\n",
    "                correct_doc += 1\n",
    "\n",
    "    action_acc = correct_action/total_len\n",
    "    if(transferencia_len == 0 and agregar_contacto_len == 0):\n",
    "        alias_acc = nombre_acc = cuenta_acc = entidad_acc = doc_acc = monto_acc = moneda_acc = -1\n",
    "    else:\n",
    "        alias_acc = (correct_alias/(transferencia_len+agregar_contacto_len))\n",
    "        nombre_acc = (correct_nombre/(transferencia_len+agregar_contacto_len))\n",
    "        monto_acc = (correct_monto/(transferencia_len+agregar_contacto_len))\n",
    "        entidad_acc = (correct_ent/(transferencia_len+agregar_contacto_len))\n",
    "        cuenta_acc = (correct_cuenta/(transferencia_len+agregar_contacto_len))\n",
    "        moneda_acc = (correct_moneda/(transferencia_len+agregar_contacto_len))\n",
    "        doc_acc = (correct_doc/(transferencia_len+agregar_contacto_len))\n",
    "\n",
    "    result = {\n",
    "        \"action_acc\": action_acc,\n",
    "        \"alias_acc\": alias_acc,\n",
    "        \"nombre_acc\": nombre_acc,\n",
    "        \"monto_acc\": monto_acc,\n",
    "        \"moneda_acc\": moneda_acc,\n",
    "        \"cuenta_acc\": cuenta_acc,\n",
    "        \"entidad_acc\": entidad_acc,\n",
    "        \"doc_acc\": doc_acc\n",
    "    }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics_with_csv_building(save_to_csv=False, csv_path=None):\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels, inputs = eval_pred\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "        # Replace -100 in the labels as we can't decode them.\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        inputs = np.where(inputs != -100, inputs, tokenizer.pad_token_id)\n",
    "        decoded_inputs = tokenizer.batch_decode(inputs, skip_special_tokens=True)\n",
    "        \n",
    "        # longest_token_len_labels = 0\n",
    "        # longest_token_len_pred = 0\n",
    "        # longest_prediction = max(decoded_preds, key=len)\n",
    "        # longest_label = max(decoded_labels, key=len)\n",
    "        # for decoded_label, decoded_pred in zip(decoded_labels, decoded_preds):\n",
    "        #     len_tokens_label = len(tokenizer.encode(decoded_label))\n",
    "        #     len_tokens_pred = len(tokenizer.encode(decoded_pred))\n",
    "        #     if len_tokens_label > longest_token_len_labels:\n",
    "        #         longest_token_len_labels = len_tokens_label\n",
    "        #     if len_tokens_pred > longest_token_len_pred:\n",
    "        #         longest_token_len_pred = len_tokens_pred\n",
    "\n",
    "        # print(f\"Longest Label: {longest_label}\")\n",
    "        # print(f\"Max Token Length Labels: {longest_token_len_labels}\")\n",
    "        # print()\n",
    "        # print(f\"Longest Prediction: {longest_prediction}\")\n",
    "        # print(f\"Max Token Length Prediction: {longest_token_len_pred}\")\n",
    "        result = {}\n",
    "\n",
    "        # Compute CER\n",
    "        result[\"cer\"] = cer.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "        \n",
    "        # Compute Exact Match\n",
    "        exact_match_res = exact_match.compute(predictions=decoded_preds, references=decoded_labels, ignore_case=True)\n",
    "        result[\"exact_match\"] = exact_match_res[\"exact_match\"]\n",
    "\n",
    "        # Compute Custom Eval\n",
    "        result.update(custom_eval(predictions=decoded_preds, references=decoded_labels))\n",
    "\n",
    "        if(result[\"exact_match\"] < 1 and save_to_csv and csv_path is not None):\n",
    "            non_matches = []\n",
    "            for input, pred, label in zip(decoded_inputs, decoded_preds, decoded_labels):\n",
    "                is_exact_match = pred.lower() == label.lower()\n",
    "                if not is_exact_match:\n",
    "                    non_matches.append({\"frase\": input, \"reference_string\": label, \"predicted_string\": pred})\n",
    "            non_matches_df = pd.DataFrame(non_matches)\n",
    "            non_matches_df.to_csv(csv_path, index=False)\n",
    "\n",
    "        return {k: round(v, 4) for k, v in result.items()}\n",
    "    return compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enzodefi/Roshka/Chimi/Coding/test_dvc/venv/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"models/chimi-mt5-base\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    include_inputs_for_metrics=True,\n",
    "    predict_with_generate=True,\n",
    "    warmup_steps=20,\n",
    "    save_total_limit=1,\n",
    "    num_train_epochs=1,\n",
    "    metric_for_best_model=\"exact_match\",\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adafactor(\n",
    "    base_model.parameters(),\n",
    "    lr=1e-3,\n",
    "    clip_threshold=1.0,\n",
    "    weight_decay=0.0,\n",
    "    relative_step=False,\n",
    "    scale_parameter=False,\n",
    "    warmup_init=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.config.max_length = 100\n",
    "base_model.config.max_length\n",
    "compute_metrics_func = compute_metrics_with_csv_building()\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=base_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_chimi_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_chimi_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    optimizers=(optimizer, None),\n",
    "    compute_metrics=compute_metrics_func\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c586cb1b1e0643b9b1132835ed8231c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 100}\n",
      "Your generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3612, 'grad_norm': 2.210242748260498, 'learning_rate': 0.0, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enzodefi/Roshka/Chimi/Coding/test_dvc/venv/lib/python3.11/site-packages/transformers/generation/utils.py:1376: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4efbd666c2554d4a931f2532aa1f98ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 100}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.30494630336761475, 'eval_cer': 0.1314, 'eval_exact_match': 0.454, 'eval_action_acc': 0.89, 'eval_alias_acc': 0.8105, 'eval_nombre_acc': 0.818, 'eval_monto_acc': 0.793, 'eval_moneda_acc': 0.808, 'eval_cuenta_acc': 0.7406, 'eval_entidad_acc': 0.8803, 'eval_doc_acc': 0.6135, 'eval_runtime': 31.316, 'eval_samples_per_second': 15.966, 'eval_steps_per_second': 2.012, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 129.9925, 'train_samples_per_second': 30.771, 'train_steps_per_second': 3.846, 'train_loss': 2.36116552734375, 'epoch': 1.0}\n",
      "Time: 129.99\n",
      "Samples/second: 30.77\n",
      "GPU memory occupied: 3536 MB.\n"
     ]
    }
   ],
   "source": [
    "result = trainer.train()\n",
    "print_summary(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Incorrect path_or_model_id: 'base_models/chimi-mt5-base/checkpoint-12000-974'. Please provide either the path to a local folder or the repo_id of a model on the Hub.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Roshka/Chimi/Coding/test_dvc/venv/lib/python3.11/site-packages/transformers/utils/hub.py:402\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Roshka/Chimi/Coding/test_dvc/venv/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:101\u001b[0m, in \u001b[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Roshka/Chimi/Coding/test_dvc/venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Roshka/Chimi/Coding/test_dvc/venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:154\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m     )\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'base_models/chimi-mt5-base/checkpoint-12000-974'. Use `repo_type` argument if needed.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 4800 98.65 | Doc 98  5000 - 98.85\u001b[39;00m\n\u001b[1;32m      2\u001b[0m trained_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase_models/chimi-mt5-base/checkpoint-12000-974\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mMT5ForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrained_model_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m trained_tokenizer \u001b[38;5;241m=\u001b[39m MT5Tokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(trained_model_path)\n",
      "File \u001b[0;32m~/Roshka/Chimi/Coding/test_dvc/venv/lib/python3.11/site-packages/transformers/modeling_utils.py:3203\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m commit_hash \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m   3202\u001b[0m         \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[0;32m-> 3203\u001b[0m         resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3204\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3205\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3206\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3207\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3208\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3209\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3210\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3211\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3212\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3213\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3214\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3215\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3216\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3217\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3218\u001b[0m         commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m   3219\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Roshka/Chimi/Coding/test_dvc/venv/lib/python3.11/site-packages/transformers/utils/hub.py:466\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was a specific connection error when trying to load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HFValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    467\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect path_or_model_id: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please provide either the path to a local folder or the repo_id of a model on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    468\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n",
      "\u001b[0;31mOSError\u001b[0m: Incorrect path_or_model_id: 'base_models/chimi-mt5-base/checkpoint-12000-974'. Please provide either the path to a local folder or the repo_id of a model on the Hub."
     ]
    }
   ],
   "source": [
    "# 4800 98.65 | Doc 98  5000 - 98.85\n",
    "trained_model_path = \"models/chimi-mt5-base/checkpoint-12000-974\"\n",
    "trained_model = MT5ForConditionalGeneration.from_pretrained(trained_model_path).to(\"cuda\")\n",
    "trained_tokenizer = MT5Tokenizer.from_pretrained(trained_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "script_dir = os.getcwd()\n",
    "errors_csv_folder_path = os.path.join(script_dir, \"errors_model_csvs\")\n",
    "os.makedirs(os.path.dirname(errors_csv_folder_path), exist_ok=True)\n",
    "csv_name = \"error_csv_check_12000.csv\"\n",
    "csv_path = os.path.join(errors_csv_folder_path, csv_name)\n",
    "\n",
    "compute_metrics_func = compute_metrics_with_csv_building(save_to_csv=True, csv_path=csv_path)\n",
    "\n",
    "test_trainer = Seq2SeqTrainer(\n",
    "    model=trained_model,\n",
    "    args=training_args,\n",
    "    eval_dataset=tokenized_chimi_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=trained_tokenizer,\n",
    "    compute_metrics=compute_metrics_func\n",
    ")\n",
    "\n",
    "test_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "el_purete = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=trained_model,\n",
    "    tokenizer=trained_tokenizer,\n",
    "    max_new_tokens=75,\n",
    "    num_beams=1,\n",
    "    device=\"cuda:0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = el_purete([\"Sera que le podes pagar 1200000 a Fissch\", \"Transferie na 350000 a Kike Fanego\", \"Epaaaaa?\", \"Sera que le podes agregar a Javier Mereles a mis contactos. Su cuenta es del Banco Continental, y su numero de cedula es 2849212\", \"Podes pasarle 55000 a Enzo Flecha\", \"Le quiero pasar 52500 a mi socio Joaquincho\", \"Nro. Cta: 0515277944\\nSolar Banco\\nC.I: 2355066\\nBlanca Nancy Martínez\", \"Vanessa Cristina Gamarra Cantero\\nCuenta Numero 346936492\\nVision Banco\\nNúmero de Documento: 7,366,864\", \"Che, sera que le podes agregar a Javier Flecha con numero de cedula 91446060. Tiene su cuenta en el BNA.\"])\n",
    "\n",
    "# result = el_purete(\"Sera que le podes pagar 1200000 a Spachuzo\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = el_purete(\"Sera que le podes agregar a Joaquín Flecha con numero de cuenta 799822551. Su cuenta la tiene en el Banco de la Nación de Argentina.\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\"Transferile na quinientos cincuenta mil a Kike Fanego\"]\n",
    "\n",
    "inputs = trained_tokenizer(inputs, max_length=200, truncation=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "output = trained_model(**inputs)\n",
    "type(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
    "\n",
    "print(decoded_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_chimi_train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
